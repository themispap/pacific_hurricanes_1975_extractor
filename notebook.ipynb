{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacific Hurricanes 1975 extractor\n",
    "Web scraper for the 1975 Pacific hurricane season Wikipedia page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1975_Pacific_hurricane_season_summary_map_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the requested page.\n"
     ]
    }
   ],
   "source": [
    "# URL for the 1975 Pacific hurricane season Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season'\n",
    "# Parse the HTML\n",
    "soup=f.get_url_content(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hurricane/storm name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_element=soup.find_all('div', {'class':'mw-heading mw-heading3'})\n",
    "hurricane_storm_name=[]\n",
    "for h3 in div_element:\n",
    "    striped_h3=h3.find('h3').text.strip()\n",
    "    hurricane_storm_name.append(striped_h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start/end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_element=soup.find_all('table', {'class':'infobox'})\n",
    "exact_class_tables = [table for table in table_element if table['class'] == ['infobox']]\n",
    "start_date=[]\n",
    "end_date=[]\n",
    "for td in exact_class_tables:\n",
    "    striped_date=td.find('td',{'class':'infobox-data'}).text.strip().split('\\xa0â€“ ')\n",
    "    start_date.append(striped_date[0])\n",
    "    end_date.append(striped_date[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of deaths and list of affected areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of each hurricane/storm paragraph\n",
    "all_h3 = soup.find_all('div',{'class':'mw-heading mw-heading3'})\n",
    "info_text=[]\n",
    "for h3 in all_h3:\n",
    "    content_between_h3 = []\n",
    "    # Loop through the siblings after the <div> tag\n",
    "    for sibling in h3.find_next_siblings(recursive=False):\n",
    "        # Stop when reaching the next <div> tag\n",
    "        if sibling.name == 'div' and sibling.attrs=={'class': ['mw-heading', 'mw-heading3']}:\n",
    "            break\n",
    "        # Append the content of <p> to the list\n",
    "        if sibling.name == 'p':\n",
    "            text=sibling.text.strip()\n",
    "            content_between_h3.append(text)\n",
    "    # Join the list into a single string\n",
    "    info_text.append(''.join(content_between_h3))\n",
    "# Drop the last list item\n",
    "info_text = info_text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the required information from the scraped text using gpt-3.5-turbo. Duration~15s\n",
    "result=[]\n",
    "tokens_used = 0\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "for text in info_text:\n",
    "    response=f.extract_info_from_text(text)\n",
    "    text_response = response.choices[0].message.content\n",
    "    result.append(text_response)\n",
    "    tokens_used += response.usage.total_tokens\n",
    "    prompt_tokens += response.usage.prompt_tokens\n",
    "    completion_tokens += response.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse the required information from the scraped text using gpt-3.5-turbo. Duration~15s\n",
    "# result=[]\n",
    "# for text in info_text:\n",
    "#     result.append(f.extract_info_from_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "deaths=[]\n",
    "affected_areas=[]\n",
    "for item in result:\n",
    "    # convert the parsed info into python dictionary format\n",
    "    item_n = item.replace(\"'\",'\"')\n",
    "    info = json.loads(item_n)\n",
    "    # extract info from the dictionary\n",
    "    deaths.append(info['number_of_deaths'])\n",
    "    affected_areas.append(info['areas_affected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean data and export to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns={'hurricane_storm_name':hurricane_storm_name[:-1],\n",
    "         'date_start':start_date,\n",
    "         'date_end':end_date,\n",
    "         'number_of_deaths':deaths,\n",
    "         'list_of_areas_affected':affected_areas\n",
    "         }\n",
    "\n",
    "# populate a dataframe with the required info\n",
    "df = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conversion to the 'date' column\n",
    "df['date_start'] = df['date_start'].apply(f.convert_date)\n",
    "df['date_end'] = df['date_end'].apply(f.convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the areas from the area list\n",
    "df['list_of_areas_affected'] = df['list_of_areas_affected'].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv\n",
    "df.to_csv('hurricanes_1975.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('log.txt', 'w') as file:\n",
    "    file.write('tokens_used: ' + str(tokens_used) + '\\n')\n",
    "    file.write('prompt_tokens: ' + str(prompt_tokens) + '\\n')\n",
    "    file.write('completion_tokens: ' + str(completion_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_extractor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
