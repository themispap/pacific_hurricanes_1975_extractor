{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacific Hurricanes 1975 extractor\n",
    "Web scraper for the 1975 Pacific hurricane season Wikipedia page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the requested page.\n"
     ]
    }
   ],
   "source": [
    "# URL for the 1975 Pacific hurricane season Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season'\n",
    "# Parse the HTML\n",
    "soup=f.get_url_content(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hurricane/storm name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_element=soup.find_all('div', {'class':'mw-heading mw-heading3'})\n",
    "hurricane_storm_name=[]\n",
    "for h3 in div_element:\n",
    "    striped_h3=h3.find('h3').text.strip()\n",
    "    hurricane_storm_name.append(striped_h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hurricane Agatha',\n",
       " 'Tropical Storm Bridget',\n",
       " 'Hurricane Carlotta',\n",
       " 'Hurricane Denise',\n",
       " 'Tropical Storm Eleanor',\n",
       " 'Tropical Storm Francene',\n",
       " 'Tropical Storm Georgette',\n",
       " 'Tropical Storm Hilary',\n",
       " 'Hurricane Ilsa',\n",
       " 'Hurricane Jewel',\n",
       " 'Hurricane Katrina',\n",
       " 'Unnamed hurricane',\n",
       " 'Hurricane Lily',\n",
       " 'Tropical Storm Monica',\n",
       " 'Tropical Storm Nanette',\n",
       " 'Hurricane Olivia',\n",
       " 'Tropical Storm Priscilla',\n",
       " 'Other systems']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hurricane_storm_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start/end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_element=soup.find_all('table', {'class':'infobox'})\n",
    "exact_class_tables = [table for table in table_element if table['class'] == ['infobox']]\n",
    "start_date=[]\n",
    "end_date=[]\n",
    "for td in exact_class_tables:\n",
    "    striped_date=td.find('td',{'class':'infobox-data'}).text.strip().split('\\xa0â€“ ')\n",
    "    start_date.append(striped_date[0])\n",
    "    end_date.append(striped_date[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['June 2', 'June 28', 'July 2', 'July 5', 'July 10', 'July 27', 'August 11', 'August 13', 'August 18', 'August 24', 'August 29', 'August 31', 'September 16', 'September 28', 'September 28', 'October 22', 'November 2']\n",
      "['June 5', 'July 3', 'July 11', 'July 15', 'July 12', 'July 30', 'August 14', 'August 17', 'August 26', 'August 31', 'September 7', 'September 5', 'September 21', 'October 2', 'October 4', 'October 25', 'November 7']\n"
     ]
    }
   ],
   "source": [
    "print(start_date)\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of deaths and list of affected areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of each hurricane/storm paragraph\n",
    "all_h3 = soup.find_all('div',{'class':'mw-heading mw-heading3'})\n",
    "info_text=[]\n",
    "for h3 in all_h3:\n",
    "    content_between_h3 = []\n",
    "    # Loop through the siblings after the <div> tag\n",
    "    for sibling in h3.find_next_siblings():\n",
    "        # Stop when reaching the next <div> tag\n",
    "        if sibling.name == 'div':\n",
    "            break\n",
    "        # Append the content of <p> to the list\n",
    "        if sibling.name == 'p':\n",
    "            text=sibling.text.strip()\n",
    "            content_between_h3.append(text)\n",
    "    # Join the list into a single string\n",
    "    info_text.append(''.join(content_between_h3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai test\n",
    "text=info_text[1]\n",
    "info=f.extract_info_from_text(text)\n",
    "info['number_of_deaths']\n",
    "info['areas_affected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths=[]\n",
    "affected_areas=[]\n",
    "for text in info_text:\n",
    "    info=f.extract_info_from_text(text)\n",
    "    deaths.append(info['number_of_deaths'])\n",
    "    affected_areas.append(info['areas_affected'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_extractor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
