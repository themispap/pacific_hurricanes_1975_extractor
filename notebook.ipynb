{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacific Hurricanes 1975 extractor\n",
    "Web scraper for the 1975 Pacific hurricane season Wikipedia page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1975_Pacific_hurricane_season_summary_map_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Required Libraries and Functions\n",
    "Here, we import the necessary libraries, including the custom `functions.py` file for utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom utility functions from the 'functions.py' file as 'f'\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping the Wikipedia Page for Pacific Hurricanes 1975\n",
    "We fetch the Wikipedia page for the 1975 Pacific hurricane season and parse its HTML content using BeautifulSoup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the requested page.\n"
     ]
    }
   ],
   "source": [
    "# URL for the 1975 Pacific hurricane season Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season'\n",
    "\n",
    "# Call function to get and parse the HTML content of the URL\n",
    "soup = f.get_url_content(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting Hurricane and Storm Names\n",
    "We search the parsed HTML for hurricane and storm names in the appropriate `<div>` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <div> elements containing class 'mw-heading mw-heading3' (hurricane/storm names)\n",
    "div_element = soup.find_all('div', {'class': 'mw-heading mw-heading3'})\n",
    "\n",
    "# Initialize a list to hold storm/hurricane names\n",
    "hurricane_storm_name = []\n",
    "\n",
    "# Loop through each div element to extract storm names\n",
    "for h3 in div_element:\n",
    "    striped_h3 = h3.find('h3').text.strip()  # Extract and clean text of the storm name\n",
    "    hurricane_storm_name.append(striped_h3)  # Append the cleaned name to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extracting Start and End Dates for Each Hurricane/Storm\n",
    "We locate the tables that contain the start and end dates for each hurricane or storm and store them in lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <table> elements with class 'infobox' (information boxes)\n",
    "table_element = soup.find_all('table', {'class': 'infobox'})\n",
    "\n",
    "# Filter tables that have exactly 'infobox' class (ignore other table types)\n",
    "exact_class_tables = [table for table in table_element if table['class'] == ['infobox']]\n",
    "\n",
    "# Initialize lists to hold start and end dates\n",
    "start_date = []\n",
    "end_date = []\n",
    "\n",
    "# Loop through the filtered infobox tables to extract storm dates\n",
    "for td in exact_class_tables:\n",
    "    # Extract the start and end dates of the storm (strip extra characters like '\\xa0–')\n",
    "    striped_date = td.find('td', {'class': 'infobox-data'}).text.strip().split('\\xa0– ')\n",
    "    start_date.append(striped_date[0])  # Append start date\n",
    "    end_date.append(striped_date[1])    # Append end date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extracting Descriptions for Each Hurricane/Storm\n",
    "We now scrape the paragraphs of information related to each storm and store them in a list for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold storm/hurricane descriptions\n",
    "info_text = []\n",
    "\n",
    "# Find all <div> elements with class 'mw-heading mw-heading3' for each storm's description\n",
    "all_h3 = soup.find_all('div', {'class': 'mw-heading mw-heading3'})\n",
    "\n",
    "# Loop through each heading and extract its corresponding paragraphs\n",
    "for h3 in all_h3:\n",
    "    content_between_h3 = []\n",
    "    \n",
    "    # Loop through sibling tags after each <div> tag until another <div> tag is found\n",
    "    for sibling in h3.find_next_siblings(recursive=False):\n",
    "        # Break the loop if another heading <div> is found\n",
    "        if sibling.name == 'div' and sibling.attrs == {'class': ['mw-heading', 'mw-heading3']}:\n",
    "            break\n",
    "        # Collect text from <p> (paragraphs)\n",
    "        if sibling.name == 'p':\n",
    "            text = sibling.text.strip()  # Clean and extract paragraph text\n",
    "            content_between_h3.append(text)\n",
    "    \n",
    "    # Join paragraphs into a single string\n",
    "    info_text.append(''.join(content_between_h3))\n",
    "\n",
    "# Remove the last element from the info_text list\n",
    "info_text = info_text[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Using GPT to Extract Structured Information from the Descriptions\n",
    "We use the OpenAI GPT model to extract the number of deaths and areas affected by each hurricane/storm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold extracted information and token counts\n",
    "result = []\n",
    "tokens_used = 0\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "\n",
    "# Loop through each hurricane description and extract information using the GPT function\n",
    "for text in info_text:\n",
    "    response = f.extract_info_from_text(text)  # Extract data using OpenAI GPT\n",
    "    text_response = response.choices[0].message.content  # Get GPT response\n",
    "    result.append(text_response)  # Append extracted data to the result list\n",
    "    \n",
    "    # Track usage statistics for OpenAI API tokens\n",
    "    tokens_used += response.usage.total_tokens\n",
    "    prompt_tokens += response.usage.prompt_tokens\n",
    "    completion_tokens += response.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Processing the Extracted Information\n",
    "We process the GPT-extracted data, converting it into a usable format (Python dictionaries) and extract relevant details such as the number of deaths and areas affected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON to handle the response data\n",
    "import json\n",
    "\n",
    "# Initialize lists to hold extracted deaths and affected areas data\n",
    "deaths = []\n",
    "affected_areas = []\n",
    "\n",
    "# Loop through each GPT response and convert it to a Python dictionary\n",
    "for item in result:\n",
    "    # Replace single quotes with double quotes to ensure valid JSON format\n",
    "    item_n = item.replace(\"'\", '\"')\n",
    "    info = json.loads(item_n)  # Convert string to dictionary\n",
    "    \n",
    "    # Extract deaths and affected areas from the dictionary\n",
    "    deaths.append(info['number_of_deaths'])\n",
    "    affected_areas.append(info['areas_affected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Creating a DataFrame\n",
    "We create a pandas DataFrame to organize the hurricane information, including storm names, dates, deaths, and affected areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to handle dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with all columns (storm names, dates, deaths, areas)\n",
    "columns = {\n",
    "    'hurricane_storm_name': hurricane_storm_name[:-1],  # Exclude the last entry\n",
    "    'date_start': start_date,\n",
    "    'date_end': end_date,\n",
    "    'number_of_deaths': deaths,\n",
    "    'list_of_areas_affected': affected_areas\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Formatting Dates and Areas\n",
    "We apply additional formatting to the start and end dates and combine the list of affected areas into a single string for each storm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the date conversion function to start and end dates\n",
    "df['date_start'] = df['date_start'].apply(f.convert_date)\n",
    "df['date_end'] = df['date_end'].apply(f.convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the list of areas affected into a single string for each storm\n",
    "df['list_of_areas_affected'] = df['list_of_areas_affected'].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hurricane_storm_name</th>\n",
       "      <th>date_start</th>\n",
       "      <th>date_end</th>\n",
       "      <th>number_of_deaths</th>\n",
       "      <th>list_of_areas_affected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hurricane Agatha</td>\n",
       "      <td>1975-06-02</td>\n",
       "      <td>1975-06-05</td>\n",
       "      <td>0</td>\n",
       "      <td>Acapulco, Zihuatanejo, Tres Marias Islands, Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tropical Storm Bridget</td>\n",
       "      <td>1975-06-28</td>\n",
       "      <td>1975-07-03</td>\n",
       "      <td>0</td>\n",
       "      <td>Baja California Peninsula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hurricane Carlotta</td>\n",
       "      <td>1975-07-02</td>\n",
       "      <td>1975-07-11</td>\n",
       "      <td>0</td>\n",
       "      <td>Acapulco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hurricane Denise</td>\n",
       "      <td>1975-07-05</td>\n",
       "      <td>1975-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>parts of Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tropical Storm Eleanor</td>\n",
       "      <td>1975-07-10</td>\n",
       "      <td>1975-07-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Acapulco, Manzanillo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tropical Storm Francene</td>\n",
       "      <td>1975-07-27</td>\n",
       "      <td>1975-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>northwestward, west</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tropical Storm Georgette</td>\n",
       "      <td>1975-08-11</td>\n",
       "      <td>1975-08-14</td>\n",
       "      <td>0</td>\n",
       "      <td>Cabo San Lucas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tropical Storm Hilary</td>\n",
       "      <td>1975-08-13</td>\n",
       "      <td>1975-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>No areas were affected as the tropical storm d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hurricane Ilsa</td>\n",
       "      <td>1975-08-18</td>\n",
       "      <td>1975-08-26</td>\n",
       "      <td>0</td>\n",
       "      <td>Gulf of Tehuantepec, Pacific Ocean, Atlantic O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hurricane Jewel</td>\n",
       "      <td>1975-08-24</td>\n",
       "      <td>1975-08-31</td>\n",
       "      <td>0</td>\n",
       "      <td>Acapulco, Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hurricane Katrina</td>\n",
       "      <td>1975-08-29</td>\n",
       "      <td>1975-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>Socorro Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Unnamed hurricane</td>\n",
       "      <td>1975-08-31</td>\n",
       "      <td>1975-09-05</td>\n",
       "      <td>0</td>\n",
       "      <td>Hawaii, Kauai, Alaska, British Columbia, Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hurricane Lily</td>\n",
       "      <td>1975-09-16</td>\n",
       "      <td>1975-09-21</td>\n",
       "      <td>0</td>\n",
       "      <td>Manzanillo, Socorro Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tropical Storm Monica</td>\n",
       "      <td>1975-09-28</td>\n",
       "      <td>1975-10-02</td>\n",
       "      <td>0</td>\n",
       "      <td>eastern Pacific Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tropical Storm Nanette</td>\n",
       "      <td>1975-09-28</td>\n",
       "      <td>1975-10-04</td>\n",
       "      <td>0</td>\n",
       "      <td>out to sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hurricane Olivia</td>\n",
       "      <td>1975-10-22</td>\n",
       "      <td>1975-10-25</td>\n",
       "      <td>30</td>\n",
       "      <td>Mazatlán</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tropical Storm Priscilla</td>\n",
       "      <td>1975-11-02</td>\n",
       "      <td>1975-11-07</td>\n",
       "      <td>0</td>\n",
       "      <td>Clarion Island</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hurricane_storm_name  date_start    date_end  number_of_deaths  \\\n",
       "0           Hurricane Agatha  1975-06-02  1975-06-05                 0   \n",
       "1     Tropical Storm Bridget  1975-06-28  1975-07-03                 0   \n",
       "2         Hurricane Carlotta  1975-07-02  1975-07-11                 0   \n",
       "3           Hurricane Denise  1975-07-05  1975-07-15                 0   \n",
       "4     Tropical Storm Eleanor  1975-07-10  1975-07-12                 0   \n",
       "5    Tropical Storm Francene  1975-07-27  1975-07-30                 0   \n",
       "6   Tropical Storm Georgette  1975-08-11  1975-08-14                 0   \n",
       "7      Tropical Storm Hilary  1975-08-13  1975-08-17                 0   \n",
       "8             Hurricane Ilsa  1975-08-18  1975-08-26                 0   \n",
       "9            Hurricane Jewel  1975-08-24  1975-08-31                 0   \n",
       "10         Hurricane Katrina  1975-08-29  1975-09-07                 0   \n",
       "11         Unnamed hurricane  1975-08-31  1975-09-05                 0   \n",
       "12            Hurricane Lily  1975-09-16  1975-09-21                 0   \n",
       "13     Tropical Storm Monica  1975-09-28  1975-10-02                 0   \n",
       "14    Tropical Storm Nanette  1975-09-28  1975-10-04                 0   \n",
       "15          Hurricane Olivia  1975-10-22  1975-10-25                30   \n",
       "16  Tropical Storm Priscilla  1975-11-02  1975-11-07                 0   \n",
       "\n",
       "                               list_of_areas_affected  \n",
       "0   Acapulco, Zihuatanejo, Tres Marias Islands, Sa...  \n",
       "1                           Baja California Peninsula  \n",
       "2                                            Acapulco  \n",
       "3                                     parts of Mexico  \n",
       "4                                Acapulco, Manzanillo  \n",
       "5                                 northwestward, west  \n",
       "6                                      Cabo San Lucas  \n",
       "7   No areas were affected as the tropical storm d...  \n",
       "8   Gulf of Tehuantepec, Pacific Ocean, Atlantic O...  \n",
       "9                                    Acapulco, Mexico  \n",
       "10                                     Socorro Island  \n",
       "11   Hawaii, Kauai, Alaska, British Columbia, Montana  \n",
       "12                         Manzanillo, Socorro Island  \n",
       "13                              eastern Pacific Ocean  \n",
       "14                                         out to sea  \n",
       "15                                           Mazatlán  \n",
       "16                                     Clarion Island  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Exporting Data to CSV\n",
    "The cleaned and formatted hurricane data is exported to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('hurricanes_1975.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Logging Token Usage\n",
    "We log the total tokens used during the GPT API calls, including prompt and completion tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the token usage information to a log file\n",
    "with open('log.txt', 'w') as file:\n",
    "    file.write('tokens_used: ' + str(tokens_used) + '\\n')\n",
    "    file.write('prompt_tokens: ' + str(prompt_tokens) + '\\n')\n",
    "    file.write('completion_tokens: ' + str(completion_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_extractor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
