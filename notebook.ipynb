{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacific Hurricanes 1975 extractor\n",
    "Web scraper for the 1975 Pacific hurricane season Wikipedia page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1975_Pacific_hurricane_season_summary_map_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Required Libraries and Functions\n",
    "Here, we import the necessary libraries, including the custom `functions.py` file for utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom utility functions from the 'functions.py' file as 'f'\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping the Wikipedia Page for Pacific Hurricanes 1975\n",
    "We fetch the Wikipedia page for the 1975 Pacific hurricane season and parse its HTML content using BeautifulSoup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the requested page.\n"
     ]
    }
   ],
   "source": [
    "# URL for the 1975 Pacific hurricane season Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season'\n",
    "\n",
    "# Call function to get and parse the HTML content of the URL\n",
    "soup = f.get_url_content(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting Hurricane and Storm Names\n",
    "We search the parsed HTML for hurricane and storm names in the appropriate `<div>` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <div> elements containing class 'mw-heading mw-heading3' (hurricane/storm names)\n",
    "div_element = soup.find_all('div', {'class': 'mw-heading mw-heading3'})\n",
    "\n",
    "# Initialize a list to hold storm/hurricane names\n",
    "hurricane_storm_name = []\n",
    "\n",
    "# Loop through each div element to extract storm names\n",
    "for h3 in div_element:\n",
    "    striped_h3 = h3.find('h3').text.strip()  # Extract and clean text of the storm name\n",
    "    hurricane_storm_name.append(striped_h3)  # Append the cleaned name to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extracting Start and End Dates for Each Hurricane/Storm\n",
    "We locate the tables that contain the start and end dates for each hurricane or storm and store them in lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <table> elements with class 'infobox' (information boxes)\n",
    "table_element = soup.find_all('table', {'class': 'infobox'})\n",
    "\n",
    "# Filter tables that have exactly 'infobox' class (ignore other table types)\n",
    "exact_class_tables = [table for table in table_element if table['class'] == ['infobox']]\n",
    "\n",
    "# Initialize lists to hold start and end dates\n",
    "start_date = []\n",
    "end_date = []\n",
    "\n",
    "# Loop through the filtered infobox tables to extract storm dates\n",
    "for td in exact_class_tables:\n",
    "    # Extract the start and end dates of the storm (strip extra characters like '\\xa0–')\n",
    "    striped_date = td.find('td', {'class': 'infobox-data'}).text.strip().split('\\xa0– ')\n",
    "    start_date.append(striped_date[0])  # Append start date\n",
    "    end_date.append(striped_date[1])    # Append end date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extracting Descriptions for Each Hurricane/Storm\n",
    "We now scrape the paragraphs of information related to each storm and store them in a list for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold storm/hurricane descriptions\n",
    "info_text = []\n",
    "\n",
    "# Find all <div> elements with class 'mw-heading mw-heading3' for each storm's description\n",
    "all_h3 = soup.find_all('div', {'class': 'mw-heading mw-heading3'})\n",
    "\n",
    "# Loop through each heading and extract its corresponding paragraphs\n",
    "for h3 in all_h3:\n",
    "    content_between_h3 = []\n",
    "    \n",
    "    # Loop through sibling tags after each <div> tag until another <div> tag is found\n",
    "    for sibling in h3.find_next_siblings(recursive=False):\n",
    "        # Break the loop if another heading <div> is found\n",
    "        if sibling.name == 'div' and sibling.attrs == {'class': ['mw-heading', 'mw-heading3']}:\n",
    "            break\n",
    "        # Collect text from <p> (paragraphs)\n",
    "        if sibling.name == 'p':\n",
    "            text = sibling.text.strip()  # Clean and extract paragraph text\n",
    "            content_between_h3.append(text)\n",
    "    \n",
    "    # Join paragraphs into a single string\n",
    "    info_text.append(''.join(content_between_h3))\n",
    "\n",
    "# Remove the last element from the info_text list\n",
    "info_text = info_text[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Using GPT to Extract Structured Information from the Descriptions\n",
    "We use the OpenAI GPT model to extract the number of deaths and areas affected by each hurricane/storm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold extracted information and token counts\n",
    "result = []\n",
    "tokens_used = 0\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "\n",
    "# Loop through each hurricane description and extract information using the GPT function\n",
    "for text in info_text:\n",
    "    response = f.extract_info_from_text(text)  # Extract data using OpenAI GPT\n",
    "    text_response = response.choices[0].message.content  # Get GPT response\n",
    "    result.append(text_response)  # Append extracted data to the result list\n",
    "    \n",
    "    # Track usage statistics for OpenAI API tokens\n",
    "    tokens_used += response.usage.total_tokens\n",
    "    prompt_tokens += response.usage.prompt_tokens\n",
    "    completion_tokens += response.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Processing the Extracted Information\n",
    "We process the GPT-extracted data, converting it into a usable format (Python dictionaries) and extract relevant details such as the number of deaths and areas affected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON to handle the response data\n",
    "import json\n",
    "\n",
    "# Initialize lists to hold extracted deaths and affected areas data\n",
    "deaths = []\n",
    "affected_areas = []\n",
    "\n",
    "# Loop through each GPT response and convert it to a Python dictionary\n",
    "for item in result:\n",
    "    # Replace single quotes with double quotes to ensure valid JSON format\n",
    "    item_n = item.replace(\"'\", '\"')\n",
    "    info = json.loads(item_n)  # Convert string to dictionary\n",
    "    \n",
    "    # Extract deaths and affected areas from the dictionary\n",
    "    deaths.append(info['number_of_deaths'])\n",
    "    affected_areas.append(info['areas_affected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Creating a DataFrame\n",
    "We create a pandas DataFrame to organize the hurricane information, including storm names, dates, deaths, and affected areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to handle dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with all columns (storm names, dates, deaths, areas)\n",
    "columns = {\n",
    "    'hurricane_storm_name': hurricane_storm_name[:-1],  # Exclude the last entry\n",
    "    'date_start': start_date,\n",
    "    'date_end': end_date,\n",
    "    'number_of_deaths': deaths,\n",
    "    'list_of_areas_affected': affected_areas\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Formatting Dates and Areas\n",
    "We apply additional formatting to the start and end dates and combine the list of affected areas into a single string for each storm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the date conversion function to start and end dates\n",
    "df['date_start'] = df['date_start'].apply(f.convert_date)\n",
    "df['date_end'] = df['date_end'].apply(f.convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the list of areas affected into a single string for each storm\n",
    "df['list_of_areas_affected'] = df['list_of_areas_affected'].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Exporting Data to CSV\n",
    "The cleaned and formatted hurricane data is exported to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('hurricanes_1975.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Logging Token Usage\n",
    "We log the total tokens used during the GPT API calls, including prompt and completion tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the token usage information to a log file\n",
    "with open('log.txt', 'w') as file:\n",
    "    file.write('tokens_used: ' + str(tokens_used) + '\\n')\n",
    "    file.write('prompt_tokens: ' + str(prompt_tokens) + '\\n')\n",
    "    file.write('completion_tokens: ' + str(completion_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_extractor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
